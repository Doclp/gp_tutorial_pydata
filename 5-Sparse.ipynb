{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Gaussian processes\n",
    "\n",
    "One of the major constraints that limits the utility of Gaussian processes in practice is the inversion of $K$ when calculating the posterior covariance. Since it is evaluated at every observed data point, its execution time is $\\mathcal{O(n^3)}$, which makes Gaussian processes (in the form I have presented here) impractical for larger datasets.\n",
    "\n",
    "An approach for dealing with this computation complexity is to look for an approximation to accelerate training and prediction. For Gaussian processes, this can be accomplished by employing a **sparse approximation** to the Gram matrix that places $M<<N$ *inducing points* along the range of the input variables, and uses this to estimate the full covariance matrix for the observed points. \n",
    "\n",
    "The `gp.MarginalSparse` class implements sparse, or inducing point, GP approximations.  It works identically to `gp.Marginal`, except it additionally requires the locations of the inducing points (denoted `Xu`), and it accepts the argument `sigma` instead of `noise` because these sparse approximations assume white IID noise.\n",
    "\n",
    "The downside of sparse approximations is that they reduce the expressiveness of the GP.  Reducing the dimension of the covariance matrix effectively reduces the number of covariance matrix eigenvectors that can be used to fit the data.  \n",
    "\n",
    "A choice that needs to be made is where to place the inducing points.  One option is to use a subset of the inputs.  Another possibility is to use K-means.  The location of the inducing points can also be an unknown and optimized as part of the model.  These sparse approximations are useful for speeding up calculations when the density of data points is high and the lengthscales is larger than the separations between inducing points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense dataset\n",
    "\n",
    "For the following examples, we use the same data set as was used in the `gp.Marginal` example, but with more data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 2000 # The number of data points\n",
    "X = 10*np.sort(np.random.rand(n))[:,None]\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ℓ_true = 1.0\n",
    "η_true = 3.0\n",
    "cov_func = η_true**2 * pm.gp.cov.Matern52(1, ℓ_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC3 built on top of Theano\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "σ_true = 2.0\n",
    "y = f_true + σ_true * np.random.randn(n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"The true f(x)\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify a sparse marginal likelihood model via `MarginalSparse`, where the approximation method can be chosen. We will use the **fully independent training conditional (FITC)** algorithm to estimate the model, with the critical approximation being the imposition of a conditional independence assumption on the joint prior over training and test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    \n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "    \n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    gp = pm.gp.MarginalSparse(cov_func=cov, approx=\"FITC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next need a set of inducing points. We will initialize 20 inducing points with the **K-means** algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    Xu = pm.gp.util.kmeans_inducing_points(20, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we use the `marginal_likelihood` method, just as we did with the full GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with sparse_model:\n",
    "    \n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, Xu=Xu, y=y, sigma=σ)\n",
    "    \n",
    "    trace = pm.sample(1000, njobs=1, chains=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(-1, 11, 200)[:,None]\n",
    "\n",
    "# add the GP conditional to the model, given the new X values\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)\n",
    "\n",
    "# To use the MAP values, you can just replace the trace with a length-1 list with `mp`\n",
    "with sparse_model:\n",
    "    pred_samples = pm.sample_ppc(trace, vars=[f_pred], samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, pred_samples[\"f_pred\"], X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(Xu, 10*np.ones(Xu.shape[0]), \"co\", ms=10, label=\"Inducing point locations\")\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional GP\n",
    "\n",
    "Until now, our examples have been of 1-dimensional Gaussian processes, where there is just a single predictor variable thought to have a non-linear relationship to the outcome. Let's look at a real-world dataset that involves two predictors. We will use the famous **Walker Lake dataset (Isaaks & Srivistava 1989)** that involves spatial sampling of minerals and other variables over space. The data consist of two spatial coordinates and three measured outcomes. The outcomes are anonymously labeled as U, V (continuous variables, such as concentrarion) and T (discrete variable, such as the presence of a particular element). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "walker_data = pd.read_table('../data/walker.txt', sep='\\s+', index_col=0, skiprows=8, header=None, \n",
    "              names=['ID', 'Xloc', 'Yloc', 'V', 'U', 'T'])\n",
    "walker_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The samples are taken regularly over a coarse grid across the entire area, and then irregularly over portions of the area, presumably where there were positive samples on the coarser grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx = 40\n",
    "x1, x2 = np.meshgrid(np.linspace(0,300,nx), np.linspace(0,300,nx))\n",
    "X = np.concatenate([x1.reshape(nx*nx, 1), x2.reshape(nx*nx, 1)], 1)\n",
    "\n",
    "X_obs = walker_data[['Xloc', 'Yloc']].values\n",
    "y_obs = walker_data.V.values\n",
    "\n",
    "with sns.axes_style(\"white\"):\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(X_obs[:,0], X_obs[:,1], s=50, c=y_obs, marker='s', cmap=plt.cm.viridis);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a grid to predict on, as well as a sparser grid of inducing points:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nd = 30\n",
    "z1, z2 = np.meshgrid(np.linspace(0, 300, nd), np.linspace(0, 300, nd))\n",
    "Z = np.concatenate([z1.reshape(nd*nd, 1), z2.reshape(nd*nd, 1)], 1)\n",
    "\n",
    "nd = 15\n",
    "xu1, xu2 = np.meshgrid(np.linspace(0, 300, nd), np.linspace(0, 300, nd))\n",
    "Xu = np.concatenate([xu1.reshape(nd*nd, 1), xu2.reshape(nd*nd, 1)], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as spatial_model:\n",
    "    \n",
    "    l = pm.HalfCauchy(\"l\", beta=3, shape=(2,))\n",
    "    sf2 = pm.HalfCauchy(\"sf2\", beta=3)\n",
    "    sn2 = pm.HalfCauchy(\"sn2\", beta=3)\n",
    "\n",
    "    K = pm.gp.cov.ExpQuad(2, l) * sf2**2\n",
    "    \n",
    "    gp_spatial = pm.gp.MarginalSparse(cov_func=K, approx=\"FITC\")\n",
    "    obs = gp_spatial.marginal_likelihood(\"obs\", X=X_obs, Xu=Xu, y=y_obs, sigma=sn2)\n",
    "\n",
    "    mp = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with spatial_model:\n",
    "\n",
    "    f_pred = gp_spatial.conditional('f_pred', Z)\n",
    "    \n",
    "    samples = pm.sample_ppc([mp], vars=[f_pred], samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with sns.axes_style(\"white\"):\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.scatter(z1.flatten(), z2.flatten(), s=100, c=samples['f_pred'].mean(0), marker='s', cmap=plt.cm.viridis);"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
