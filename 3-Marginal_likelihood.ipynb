{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Likelihood Implementation\n",
    "\n",
    "The `gp.Marginal` class in PyMC3 implements the simplest case of GP regression:  the observed data are the sum of a GP and Gaussian noise.  `gp.Marginal` has a `marginal_likelihood` method, a `conditional` method, and a `predict` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "The observations $y$ are the unknown function plus noise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\epsilon &\\sim N(0, \\Sigma) \\\\\n",
    "  y &= f(x) + \\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### The Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
    "\n",
    "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
    "\n",
    "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
    "\n",
    "**GP prior**:\n",
    "\n",
    "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
    "\n",
    "**Gaussian likelihood**:\n",
    "\n",
    "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
    "\n",
    "**Marginal likelihood**:\n",
    "\n",
    "$$\\log p(y|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K + \\sigma^2I| - \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y $$\n",
    "\n",
    "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
    "\n",
    "### Choosing parameters\n",
    "\n",
    "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
    "\n",
    "For example, in the squared exponential covariance function, we must choose two parameters:\n",
    "\n",
    "$$k(x,x^{\\prime}) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)$$\n",
    "\n",
    "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^{\\prime}$; smaller values will tend to smooth the function relative to larger values.\n",
    "\n",
    "We can use the **marginal likelihood** to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general idea about covariance functions, let's begin by defining one for our first model.\n",
    "\n",
    "We can use a Matern(5/2) covariance to model our simulated data, and pass this as the `cov_func` argument to the `Marginal` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "with pm.Model() as model:\n",
    "    \n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "    \n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    mean = pm.gp.mean.Constant(c=1)\n",
    "    gp = pm.gp.Marginal(mean_func=mean, cov_func=cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.marginal_likelihood` method\n",
    "\n",
    "The unknown latent function can be analytically integrated out of the product of the GP prior probability with a normal likelihood.  This quantity is called the marginal likelihood. \n",
    "\n",
    "$$\n",
    "p(y \\mid x) = \\int p(y \\mid f, x) \\, p(f \\mid x) \\, df\n",
    "$$\n",
    "\n",
    "The log of the marginal likelihood, $p(y \\mid x)$, is\n",
    "\n",
    "$$\n",
    "\\log p(y \\mid x) = \n",
    "  -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m}_x)^{T} \n",
    "               (\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)^{-1} \n",
    "               (\\mathbf{y} - \\mathbf{m}_x)\n",
    "  - \\frac{1}{2}|\\mathbf{K}_{xx} + \\boldsymbol\\Sigma|\n",
    "  - \\frac{n}{2}\\log (2 \\pi)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol\\Sigma$ is the covariance matrix of the Gaussian noise.  Since the Gaussian noise doesn't need to be white to be conjugate, the `marginal_likelihood` method supports either using a white noise term when a scalar is provided, or a noise covariance function when a covariance function is provided.\n",
    "\n",
    "The `gp.marginal_likelihood` method implements the quantity given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = x.reshape(-1, 1)\n",
    "\n",
    "with model:\n",
    "    \n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X, y=y, noise=σ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    marginal_post = pm.find_MAP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can collect the results into a pandas dataframe to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame({\"Parameter\": [\"ℓ\", \"η\", \"σ\"], \n",
    "              \"Value at MAP\": [float(marginal_post[\"ℓ\"]), float(marginal_post[\"η\"]), float(marginal_post[\"σ\"])],\n",
    "              \"True value\": [ℓ_true, η_true, σ_true]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.conditional` distribution\n",
    "\n",
    "In addition to fitting the model, we would like to be able to generate predictions. This implies sampling from the posterior predictive distribution, which if you recall is just some linear algebra:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "PyMC3 allows for predictive sampling after the model is fit, using the recorded values of the model parameters to generate samples. The `conditional` method implements the predictive GP above, called with a grid of points over which to generate realizations:\n",
    "\n",
    "The `.conditional` has an optional flag for `pred_noise`, which defaults to `False`.  When `pred_noise=False`, the `conditional` method produces the predictive distribution for the underlying function represented by the GP.  When `pred_noise=True`, the `conditional` method produces the predictive distribution for the GP plus noise.  \n",
    "\n",
    "If using an additive GP model, the conditional distribution for individual components can be constructed by setting the optional argument `given`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a grid of new values from `x=0` to `x=20`, then add the GP conditional to the model, given the new X values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 20, 600)[:,None]\n",
    "\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw samples from the posterior predictive distribution over the specified grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    pred_samples = pm.sample_ppc([marginal_post], vars=[f_pred], samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, pred_samples[\"f_pred\"], X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction also matches the results from `gp.Latent` very closely.  What about predicting new data points?  Here we only predicted $f_*$, not $f_*$ + noise, which is what we actually observe.\n",
    "\n",
    "The `conditional` method of `gp.Marginal` contains the flag `pred_noise` whose default value is `False`.  To draw from the *posterior predictive* distribution, we simply set this flag to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with model:\n",
    "    y_pred = gp.conditional(\"y_pred\", X_new, pred_noise=True)\n",
    "    y_samples = pm.sample_ppc([marginal_post], vars=[y_pred], samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# posterior predictive distribution\n",
    "plot_gp_dist(ax, y_samples[\"y_pred\"], X_new, plot_samples=False, palette=\"bone_r\");\n",
    "\n",
    "# overlay a scatter of one draw of random points from the \n",
    "#   posterior predictive distribution\n",
    "plt.plot(X_new, y_samples[\"y_pred\"][800, :].T, \"co\", ms=2, label=\"Predicted data\");\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"posterior predictive distribution, y_*\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world example: Spawning salmon\n",
    "\n",
    "That was contrived data; let's try applying Gaussian processes to a real problem. The plot below shows the relationship between the number of spawning salmon in a particular stream and the number of fry that are recruited into the population in the spring.\n",
    "\n",
    "We would like to model this relationship, which appears to be non-linear (we have biological knowledge that suggests it should be non-linear too).\n",
    "\n",
    "![](images/spawn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salmon_data = pd.read_table('../data/salmon.txt', sep='\\s+', index_col=0)\n",
    "salmon_data.plot.scatter(x='spawners', y='recruits', s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as salmon_model:\n",
    "\n",
    "    # Lengthscale\n",
    "    ρ = pm.HalfCauchy('ρ', 1)\n",
    "    η = pm.HalfCauchy('η', 1)\n",
    "    \n",
    "    M = pm.gp.mean.Linear(coeffs=(salmon_data.recruits/salmon_data.spawners).mean())\n",
    "    K = (η**2) * pm.gp.cov.ExpQuad(1, ρ) \n",
    "    \n",
    "    σ = pm.HalfCauchy('σ', 1)\n",
    "    \n",
    "    recruit_gp = pm.gp.Marginal(mean_func=M, cov_func=K)\n",
    "    recruit_gp.marginal_likelihood('recruits', X=salmon_data.spawners.values.reshape(-1,1), \n",
    "                           y=salmon_data.recruits.values, noise=σ)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with salmon_model:\n",
    "    salmon_trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(salmon_trace, varnames=['ρ', 'η', 'σ']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_pred = np.linspace(0, 500, 100).reshape(-1, 1)\n",
    "with salmon_model:\n",
    "    salmon_pred = recruit_gp.conditional(\"salmon_pred\", X_pred)\n",
    "    salmon_samples = pm.sample_ppc(salmon_trace, vars=[salmon_pred], samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = salmon_data.plot.scatter(x='spawners', y='recruits', c='k', s=50)\n",
    "ax.set_ylim(0, None)\n",
    "for x in salmon_samples['salmon_pred']:\n",
    "    ax.plot(X_pred, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "We might be interested in what may happen if the population gets very large -- say, 600 or 800 spawners. We can predict this, though it goes well outside the range of data that we have observed. Generate predictions from the posterior predictive distribution (via `conditional`) that covers this range of spawners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "We can use the `.predict` method to return the mean and variance given a particular `point`.  Since we used `find_MAP` in this example, `predict` returns the same mean and covariance that the distribution of `.conditional` has.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predict\n",
    "mu, var = gp.predict(X_new, point=marginal_post, diag=True)\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot mean and 2σ intervals\n",
    "plt.plot(X_new, mu, 'r', lw=2, label=\"mean and 2σ region\");\n",
    "plt.plot(X_new, mu + 2*sd, 'r', lw=1); plt.plot(X_new, mu - 2*sd, 'r', lw=1);\n",
    "plt.fill_between(X_new.flatten(), mu - 2*sd, mu + 2*sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"predictive mean and 2σ interval\"); plt.legend();"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
