{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd \n",
    "import pymc3 as pm\n",
    "import seaborn as sns\n",
    "import theano\n",
    "import theano.tensor as tt\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.cm as cmap\n",
    "sns.set_context('talk')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Implementation\n",
    "\n",
    "The `gp.Latent` class is a more general implementation of a GP.  It is called \"Latent\" because the underlying function values are treated as latent variables.  It has a `prior` method, and a `conditional` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.prior`\n",
    "\n",
    "With some data set of finite size, the `prior` method places a multivariate normal prior distribution on the vector of function values, $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\text{MvNormal}(\\mathbf{m}_{x},\\, \\mathbf{K}_{xx}) \\,,\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{m}$ and the matrix $\\mathbf{K}_{xx}$ are the mean vector and covariance matrix evaluated over the inputs $x$.  \n",
    "\n",
    "By default, PyMC3 reparameterizes the prior on `f` under the hood by rotating it with the Cholesky factor of its covariance matrix.  This helps to reduce covariances in the posterior of the transformed random variable, `v`.  The reparameterized model is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathbf{v} \\sim \\text{N}(0, 1)& \\\\\n",
    "  \\mathbf{L} = \\text{Cholesky}(\\mathbf{K}_{xx})& \\\\\n",
    "  \\mathbf{f} = \\mathbf{m}_{x} + \\mathbf{Lv} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This reparameterization can be disabled by setting the optional flag in the `prior` method, `reparameterize = False`.  The default is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust regession\n",
    "\n",
    "The following is an example showing how to specify a simple model with a GP prior using the `gp.Latent` class.  So we can verify that the inference we perform is correct, the data set is made using a draw from a GP.  This will be identical to the first example, except that the noise is Student-T distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n = 100 # The number of data points\n",
    "X = np.linspace(0, 10, n)[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "ℓ_true = 1.0\n",
    "η_true = 3.0\n",
    "cov_func = η_true**2 * pm.gp.cov.Matern52(1, ℓ_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC3 built on top of Theano\n",
    "f_true = np.random.multivariate_normal(mean_func(X).eval(), \n",
    "                                       cov_func(X).eval() + 1e-8*np.eye(n), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of T distributed noise\n",
    "# The standard deviation of the noise is `sigma`, and the degrees of freedom is `nu`\n",
    "σ_true = 2.0\n",
    "ν_true = 3.0\n",
    "y = f_true + σ_true * np.random.standard_t(ν_true, size=n)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X, y, 'ok', ms=3, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the model in PyMC3.  We use a $\\text{Gamma}(2, 1)$ prior over the lengthscale parameter, and weakly informative $\\text{HalfCauchy}(5)$ priors over the covariance function scale, and noise scale.  A $\\text{Gamma}(2, 0.1)$ prior is assigned to the degrees of freedom parameter of the noise.  Finally, a GP prior is placed on the unknown function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "    η = pm.HalfCauchy(\"η\", beta=5)\n",
    "    \n",
    "    cov = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    \n",
    "    f = gp.prior(\"f\", X=X)\n",
    "    \n",
    "    σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "    ν = pm.Gamma(\"ν\", alpha=2, beta=0.1)\n",
    "    y_ = pm.StudentT(\"y\", mu=f, lam=1.0/σ, nu=ν, observed=y)\n",
    "    \n",
    "    trace = pm.sample(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the posteriors of the covariance function hyperparameters.  The red lines show the true values that were used to draw the function from the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, lines={\"η\": η_true, \"σ\": σ_true, \"ℓ\": ℓ_true, \"ν\": ν_true}, varnames=[\"η\", \"σ\", \"ℓ\", \"ν\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc3.gp.util import plot_gp_dist\n",
    "plot_gp_dist(ax, trace[\"f\"], X);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(X, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(X, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylabel(\"True f(x)\"); \n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "gist": {
   "data": {
    "description": "GP Showdown.ipynb",
    "public": true
   },
   "id": "05a9210d3ea6546e14e6fcb8fd10a906"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
